{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRj6FqLOG-g-",
        "outputId": "ac92fe41-93e7-42d4-c5fa-192a92e7c732"
      },
      "outputs": [],
      "source": [
        "#!pip install pytesseract\n",
        "#!pip install pypdf2\n",
        "#!pip install pdf2image\n",
        "\n",
        "!pip install tika\n",
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcjIEpuOIbdm"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rorGUmzP8Zdx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from spacy.matcher import Matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYEEI7ZqKBAL",
        "outputId": "d3ad45fb-9205-433d-94b0-6df775525bb1"
      },
      "outputs": [],
      "source": [
        "from tika import parser\n",
        "resume = r'disini file.pdf'\n",
        "resume_data = parser.from_file(resume)\n",
        "\n",
        "text = resume_data['content']\n",
        "print(text)\n",
        "\n",
        "parsed_content = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TukXWOS4KBLN",
        "outputId": "65d97c34-453a-4b72-d9ef-2858fbdfc01e"
      },
      "outputs": [],
      "source": [
        "#e-mail\n",
        "def email(string):\n",
        "    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
        "    return r.findall(string)\n",
        "\n",
        "email = email(text)\n",
        "print(\"e-mail:\", email)\n",
        "parsed_content['E-mail'] = email\n",
        "\n",
        "#phone number\n",
        "def phone_number(string):\n",
        "    r = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n",
        "    phone_numbers = r.findall(string)\n",
        "    return [re.sub(r'\\D', '', number) for number in phone_numbers]\n",
        "\n",
        "phone_number = phone_number(text)\n",
        "print(\"phone number:\", phone_number)\n",
        "parsed_content['Phone number'] = phone_number\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gth5RFXeKBRc",
        "outputId": "6f154720-7b99-47ba-e1cb-50d544f78207"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "def get_name(text):\n",
        "   nlp_text = nlp(text)\n",
        "   pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
        "  \n",
        "   matcher.add('NAME', [pattern], on_match = None)\n",
        "  \n",
        "   matches = matcher(nlp_text)\n",
        "  \n",
        "   for match_id, start, end in matches:\n",
        "       span = nlp_text[start:end]\n",
        "       return span.text\n",
        "\n",
        "name = get_name(text)\n",
        "print(\"name/sector:\", name)\n",
        "parsed_content['Name'] =  name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wmZo7KSti3w",
        "outputId": "b2bf5428-c7a1-4e60-f6cf-0b70db74043e"
      },
      "outputs": [],
      "source": [
        "text = text.replace(\"\\n\",\" \")\n",
        "text = text.replace(\"[^a-zA-Z0-9]\", \" \");  \n",
        "re.sub('\\W+','', text)\n",
        "text = text.lower()\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYPFfDFrtlFu",
        "outputId": "a60e2561-301d-44d3-b7be-b1925b24da6e"
      },
      "outputs": [],
      "source": [
        "Keywords = [\"education\",\"summary\",\"personal profile\",\n",
        "            \"work background\",\"qualifications\",\"experience\",\n",
        "            \"skills\",\"achievements\",\"projects\"]\n",
        "\n",
        "content = {}\n",
        "indices = []\n",
        "keys = []\n",
        "for key in Keywords:\n",
        "    try:\n",
        "        content[key] = text[text.index(key) + len(key):]\n",
        "        indices.append(text.index(key))\n",
        "        keys.append(key)\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "#sorting the index\n",
        "zipped_lists = zip(indices, keys)\n",
        "sorted_pairs = sorted(zipped_lists)\n",
        "sorted_pairs\n",
        "\n",
        "tuples = zip(*sorted_pairs)\n",
        "indices, keys = [ list(tuple) for tuple in  tuples]\n",
        "keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpyLvCq2tm1n"
      },
      "outputs": [],
      "source": [
        "# menghapus bagian yang berlebihan\n",
        "content = []\n",
        "for idx in range(len(indices)):\n",
        "    if idx != len(indices)-1:\n",
        "        content.append(text[indices[idx]: indices[idx+1]])\n",
        "    else:\n",
        "        content.append(text[indices[idx]: ])\n",
        "\n",
        "for i in range(len(indices)):\n",
        "    parsed_content[keys[i]] = content[i]  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRFNrfhdtnAt",
        "outputId": "85b4f704-d10d-478a-f132-731e9812ec7b"
      },
      "outputs": [],
      "source": [
        "parsed_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wbja3N0TBGf"
      },
      "outputs": [],
      "source": [
        "# print dalam bentuk df\n",
        "#pd.DataFrame(parsed_content.items())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukBMOvXW_Hod",
        "outputId": "d8676e45-8960-4fe0-e6ec-8a3cc9e49f59"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# regex\n",
        "email_regex = r'[\\w\\.-]+@[\\w\\.-]+'\n",
        "name_regex = r'Name: (.+)'\n",
        "education_regex = r'education\\s+(.+?)\\s+experience'\n",
        "experience_regex = r'experience\\s+(.+?)\\s+projects'\n",
        "projects_regex = r'projects\\s+(.+?)\\s+certifications'\n",
        "skills_regex = r'skills\\s+(.+)'\n",
        "\n",
        "# mengambil informasi dengan regex\n",
        "email_matches = re.findall(email_regex, text)\n",
        "name_match = re.search(name_regex, text)\n",
        "education_match = re.search(education_regex, text, re.DOTALL)\n",
        "experience_match = re.search(experience_regex, text, re.DOTALL)\n",
        "projects_match = re.search(projects_regex, text, re.DOTALL)\n",
        "skills_match = re.search(skills_regex, text)\n",
        "\n",
        "# informasi yang diambil\n",
        "parsed_content = {\n",
        "    'E-mail': email_matches,\n",
        "    'Name': name_match.group(1) if name_match else '',\n",
        "    'education': education_match.group(1) if education_match else '',\n",
        "    'experience': experience_match.group(1) if experience_match else '',\n",
        "    'projects': projects_match.group(1) if projects_match else '',\n",
        "    'skills': skills_match.group(1) if skills_match else ''\n",
        "}\n",
        "\n",
        "parsed_content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "rARuXIoW_Hsg",
        "outputId": "5840fab3-0632-4960-df64-f1dd55ffcc1a"
      },
      "outputs": [],
      "source": [
        "# print dalam bentuk df\n",
        "pd.DataFrame(parsed_content.items())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qSdWkVn_H1_",
        "outputId": "c48dec7f-11b3-4194-f1d3-f4a33289ee3a"
      },
      "outputs": [],
      "source": [
        "def clean_resume(parsed_content):\n",
        "    # memakai text parsed_content\n",
        "    text = parsed_content.get('education', '') + ' ' + parsed_content.get('experience', '') + ' ' + parsed_content.get('projects', '') + ' ' + parsed_content.get('skills', '')\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub('http\\S+\\s*', ' ', text)\n",
        "    # Remove RT and cc\n",
        "    text = re.sub('RT|cc', ' ', text)\n",
        "    # Remove hashtags\n",
        "    text = re.sub('#\\S+', '', text)\n",
        "    # Remove mentions\n",
        "    text = re.sub('@\\S+', ' ', text)\n",
        "    # Remove punctuations and non-ASCII characters\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "cleaned_text = clean_resume(parsed_content)\n",
        "print(cleaned_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wucTesGQH6w7",
        "outputId": "486e2a70-6ad2-4a66-fcb1-098cc491de25"
      },
      "outputs": [],
      "source": [
        "def extract_skills(text):\n",
        "    nlp_text = nlp(text)\n",
        "\n",
        "    # menghapus stopwords dan melakukan token\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "    \n",
        "    skills = [\"machine learning\",\"deep learning\",\"nlp\",\"natural language processing\",\"mysql\",\"sql\",\n",
        "              \"computer vision\",\"tensorflow\",\"opencv\",\"python\",\"matlab\"]\n",
        "   \n",
        "    setskills = []\n",
        "    for token in tokens:\n",
        "        if token.lower() in skills:\n",
        "            setskills.append(token)\n",
        "    for token in nlp_text.noun_chunks:\n",
        "        token = token.text.lower().strip()\n",
        "        if token in skills:\n",
        "            setskills.append(token)\n",
        "    return [i.capitalize() for i in set([i.lower() for i in setskills])]\n",
        "\n",
        "skills = []\n",
        "skills = extract_skills(text)\n",
        "\n",
        "parsed_content[\"Skills\"] = skills\n",
        "skills "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FD05PmpIH7Ju",
        "outputId": "9ecbb3b0-073e-4987-93ae-2653c0de6d93"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzVPu2R8qIPE",
        "outputId": "52189d37-40f5-4633-ceae-d28c21bfaff4"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # tokenisasi\n",
        "    tokens = word_tokenize(text)\n",
        "    # hapus stopwords\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    # mengembalikan ke bentuk awal\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "# terapkan stopwords\n",
        "filtered_text = remove_stopwords(cleaned_text)\n",
        "print(filtered_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmJQJK3ZqIT_"
      },
      "outputs": [],
      "source": [
        "#df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mixnt9cJ9Tw3",
        "outputId": "0bca0b80-c6fc-462d-f4a3-440efd8fed37"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qDJMIq6qIWH",
        "outputId": "9a98f38b-800b-4c12-d877-617d865d1282"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# inisialisasi stemmer dan lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_word(text):\n",
        "    # tokenissasi teks\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "    # lemmatization\n",
        "    lemmatized_words = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "    filtered_lemma_text = ' '.join(lemmatized_words)\n",
        "    return filtered_lemma_text\n",
        "\n",
        "filtered_lemma_text = lemmatize_word(cleaned_text)\n",
        "print(\"Lemmatized words:\", filtered_lemma_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ol3Wr0jrqIY4",
        "outputId": "a5185db4-fe94-4ab7-a77d-46209bb039fe"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def tokenize_text(text):\n",
        "    # tokenisasi\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "tokenized_text = tokenize_text(filtered_lemma_text)\n",
        "print(\"Tokenized words:\\n\", tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvxIyV1499sJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzGGpGDiqIbp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
